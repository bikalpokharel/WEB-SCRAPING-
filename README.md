Job Market Scraping Pipeline

Multi-Portal â€¢ Incremental UPSERT â€¢ Watch Mode â€¢ Master Builder â€¢ Quality Audit â€¢ Live Dashboard

A modular, production-style scraping and analytics pipeline built with:

Python

Selenium

pandas

Excel incremental UPSERT storage

Master dataset builder

Portal quality auditor

Dash live dashboard

Designed for continuous scraping, safe OneDrive usage, and real-time analytics.

âœ… Key Features
ğŸ”¹ Multi-Portal Architecture

Each portal lives independently inside:

portals/

Currently supported:

merojob (Selenium)

jobsnepal (Selenium)

linkedin (Rows mode / structured return)

Adding a new portal requires:

Creating portals/<newportal>.py

Implementing required functions

Registering inside run_pipeline.py

ğŸ”¹ Two Portal Modes
1ï¸âƒ£ Selenium Mode

Used for:

MeroJob

JobsNepal

Required functions:

collect_job_urls(driver, pages, limit, ...)
parse_job_detail(driver, url)

Flow:

Collect listing URLs

Visit each URL

Parse job details

2ï¸âƒ£ Rows Mode

Used for:

LinkedIn

Required function:

collect_rows(CONFIG) -> List[Dict]

Flow:

Returns normalized rows directly

UPSERT handled centrally

ğŸ”¹ Incremental UPSERT Storage

Excel files are never overwritten.

If:

Same key appears again â†’ row is updated

New key appears â†’ row is inserted

Newest scraped_at always wins.

Safe for:

Watch mode

Repeated scraping

Continuous refresh

ğŸ”¹ OneDrive-Safe Atomic Writes

To prevent corrupted Excel files:

Files are written to temp files

Then atomically replaced

Local cache used before read/write

Prevents:

BadZipFile

Partial write corruption

Network timeouts

ğŸ”¹ Watch Mode

Re-runs scraping automatically:

python run_pipeline.py --watch --interval 600 --portal all

Runs every 600 seconds.

Stop with:

Ctrl + C
ğŸ”¹ Master Dataset Builder

analysis/build_master.py

Creates:

jobs_master.xlsx

jobs_master.csv

jobs_master_local.csv (for dashboards)

Features:

Cross-portal deduplication

Global key generation

Newest record wins

Placeholder normalization

Atomic writes

ğŸ”¹ Portal Quality Audit

analysis/portal_quality.py

Generates:

portal_quality_report.xlsx

Includes:

Overall sparsity

Core field sparsity

Optional field sparsity

Missing-by-column breakdown

Per-portal sheets

Master dataset audit

ğŸ”¹ Live Dashboard (Dash)

Located in:

dashboard/

Features:

Time series of job counts

Multi-filter system

Multi-line comparison

Auto-refresh

Manual refresh

Responsive design

Reads:

jobs_master.csv
or
jobs_master_local.csv

Run:

python dashboard/app.py
ğŸ“ Project Structure
Data_scraping/
â”œâ”€â”€ config.py
â”œâ”€â”€ scraper_core.py
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ build_master.py
â”‚   â””â”€â”€ portal_quality.py
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ portals/
â”‚   â”œâ”€â”€ merojob.py
â”‚   â”œâ”€â”€ jobsnepal.py
â”‚   â””â”€â”€ linkedin.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ merojob_jobs.xlsx
â”‚   â”œâ”€â”€ jobsnepal_jobs.xlsx
â”‚   â”œâ”€â”€ linkedin_jobs.xlsx
â”‚   â””â”€â”€ _internal/
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ *.log
â”œâ”€â”€ data_local/
â”‚   â””â”€â”€ jobs_master_local.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
âš™ï¸ Requirements

Python 3.10+

Google Chrome

ChromeDriver (webdriver-manager recommended)

Install:

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
â–¶ï¸ Running the Pipeline
Run single portal
python run_pipeline.py --portal merojob
python run_pipeline.py --portal jobsnepal
python run_pipeline.py --portal linkedin
Run all portals
python run_pipeline.py --portal all
ğŸ” Watch Mode
python run_pipeline.py --watch --interval 600 --portal all
ğŸ“¦ Output Files

Per-portal:

data/<portal>_jobs.xlsx
data/_internal/<portal>_urls_latest.txt

Master:

jobs_master.xlsx
jobs_master.csv
jobs_master_local.csv

Quality:

portal_quality_report.xlsx
ğŸ§  Normalized Schema

All portal rows are normalized into:

job_id

title

company

company_link

location

country

posted_date

num_applicants

work_mode

employment_type

position

type

compensation

commitment

skills

category_primary

job_url

source

scraped_at

Master adds:

global_key

master_built_at

ğŸ” Security Notes

Avoid committing Excel data publicly

Do not hardcode credentials

LinkedIn automation may trigger rate limits

Prefer logged-in Chrome profile

Use environment variables for secrets

ğŸ›  Troubleshooting
1ï¸âƒ£ Excel corrupted (BadZipFile)

Delete corrupted file and re-run pipeline.

2ï¸âƒ£ Pandas frequency error

Use:

dt.floor("h")

(not "H")

3ï¸âƒ£ Dash run_server obsolete

Use:

app.run(...)
4ï¸âƒ£ Selenium driver crash

Driver restart logic already implemented.

ğŸš€ Advanced Capabilities (Optional Extensions)

Trend tracking of sparsity over time

Anomaly detection (row drop alerts)

JSON export for APIs

Database backend (PostgreSQL)

Docker containerization

Scheduled cron deployment

Production logging to structured logs

CI pipeline

ğŸ— System Architecture Overview

Scraper â†’ UPSERT â†’ Master Builder â†’ Quality Audit â†’ Dashboard

Portal Excel Files
        â†“
build_master.py
        â†“
jobs_master.csv
        â†“
Dash Dashboard
ğŸ Status

This pipeline is:

âœ” Production-stable
âœ” OneDrive-safe
âœ” Watch-mode safe
âœ” Incremental
âœ” Dashboard-ready
âœ” Extendable